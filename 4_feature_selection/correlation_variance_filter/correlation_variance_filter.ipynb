{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "068c9fd8-b5bb-4ba6-9d26-dd8ea5421464",
   "metadata": {},
   "source": [
    "# Filtering the Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f540019-f583-4fb6-aef9-13c1b10ad3f5",
   "metadata": {},
   "source": [
    "Remove features with a variance less than 0.2 and with correlation larger than 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46c95491-3c55-40aa-9b78-80a13362328d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "218cf9b3-1e37-457c-a0a4-df0817b66a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original data size is  (555, 1008)\n"
     ]
    }
   ],
   "source": [
    "# Read entire training dataset with all descriptors\n",
    "dataset = pd.read_csv('../../3_train_test_split/train_reg.csv').set_index('Molecule ChEMBL ID')\n",
    "print(\"The original data size is \", dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d500c1bf-200e-4ace-8d55-6fa6c86be580",
   "metadata": {},
   "source": [
    "## Remove Descriptors with Variance < 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "398829e2-d4a7-49e8-b72d-83cac7c5836a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after filtering with variance > 0.2, the data size is (555, 1004)\n"
     ]
    }
   ],
   "source": [
    "sel = VarianceThreshold(threshold=0.2) \n",
    "seldataset=sel.fit_transform(dataset)\n",
    "seldataset=dataset[dataset.columns[sel.get_support(indices=True)]]\n",
    "\n",
    "print(\"after filtering with variance > 0.2, the data size is\", seldataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c119a7-f43a-4a0c-9e02-1a11a43ea1ec",
   "metadata": {},
   "source": [
    "## Drop Features with Correlation > 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6918277-381b-4fdf-9717-da25d9f4e231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after filtering correlations > 0.95, the data size is (555, 564)\n"
     ]
    }
   ],
   "source": [
    "def drop_highcorr(df, threshold=0.95):\n",
    "    \"\"\"\n",
    "    Removes features that have a correlation higher than the specified threshold.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input DataFrame with features.\n",
    "        threshold (float): The correlation threshold above which features will be removed.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with highly correlated features removed.\n",
    "    \"\"\"\n",
    "    # Calculate the correlation matrix\n",
    "    corr_matrix = df.corr().abs()\n",
    "    \n",
    "    # Create a boolean mask for the upper triangle of the correlation matrix\n",
    "    upper_tri = corr_matrix.where(\n",
    "        np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)\n",
    "    )\n",
    "    \n",
    "    # Find columns with correlation greater than the threshold\n",
    "    to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > threshold)]\n",
    "    \n",
    "    # Drop the identified columns\n",
    "    return df.drop(columns=to_drop)\n",
    "\n",
    "\n",
    "# Apply filtration to each molecule\n",
    "filtered_df = drop_highcorr(seldataset, threshold=0.95)\n",
    "\n",
    "print(\"after filtering correlations > 0.95, the data size is\", filtered_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4ee96e-ef8a-4a3d-bdb0-88f51db1e5b1",
   "metadata": {},
   "source": [
    "## Save Train and Test Sets with Reduced Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80b3d4d9-42e3-4d9f-817a-ef99c5482346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed file saved to ./train_class_5.csv\n",
      "Processed file saved to ./train_class_4.csv\n",
      "Processed file saved to ./train_reg.csv\n",
      "Processed file saved to ./train_class_3.csv\n",
      "Processed file saved to ./train_class_2.csv\n",
      "Processed file saved to ./train_class_1.csv\n",
      "Processed file saved to ./train_reg_5.csv\n",
      "Processed file saved to ./train_reg_4.csv\n",
      "Processed file saved to ./train_reg_1.csv\n",
      "Processed file saved to ./train_reg_3.csv\n",
      "Processed file saved to ./train_reg_2.csv\n",
      "Processed file saved to ./train_class.csv\n",
      "Processed file saved to ./test_reg.csv\n",
      "Processed file saved to ./test_class.csv\n",
      "Processed file saved to ./val_class_5.csv\n",
      "Processed file saved to ./val_class_4.csv\n",
      "Processed file saved to ./val_class_1.csv\n",
      "Processed file saved to ./val_class_3.csv\n",
      "Processed file saved to ./val_class_2.csv\n",
      "Processed file saved to ./val_reg_4.csv\n",
      "Processed file saved to ./val_reg_5.csv\n",
      "Processed file saved to ./val_reg_1.csv\n",
      "Processed file saved to ./val_reg_2.csv\n",
      "Processed file saved to ./val_reg_3.csv\n"
     ]
    }
   ],
   "source": [
    "input_dir = \"../../3_train_test_split/\"\n",
    "output_dir = \".\"\n",
    "os.makedirs(output_dir, exist_ok=True)  # Ensure the output directory exists\n",
    "\n",
    "# Load descriptors_all.csv to get -logIC50\n",
    "descriptor_file = os.path.join(input_dir, \"descriptors_all.csv\")\n",
    "descriptor_df = pd.read_csv(descriptor_file)[[\"Molecule ChEMBL ID\", \"-logIC50\"]]\n",
    "\n",
    "# Columns to keep (from filtered_df)\n",
    "selected_columns = filtered_df.columns\n",
    "\n",
    "# Define the dummy variable threshold and the target column\n",
    "threshold = 1e-6\n",
    "target_column = \"-logIC50\"\n",
    "\n",
    "# Loop over train, test, and val CSV files\n",
    "for file_type in [\"train\", \"test\", \"val\"]:\n",
    "    files = glob.glob(f\"{input_dir}{file_type}*.csv\") # Find all matching CSV files for this type\n",
    "    \n",
    "    for file_path in files:\n",
    "        df = pd.read_csv(file_path).set_index('Molecule ChEMBL ID') # Read the CSV file\n",
    "        filtered_df = df[selected_columns]  # Keep only selected columns\n",
    "\n",
    "        # Add 'is_imputed' dummy variable for regression files\n",
    "        if \"reg\" in file_path:\n",
    "            merged_df = pd.merge(\n",
    "                descriptor_df, filtered_df, on=\"Molecule ChEMBL ID\", how=\"inner\"\n",
    "            ) # Merge with descriptor_df to include -logIC50\n",
    "            \n",
    "            merged_df['is_imputed'] = (\n",
    "                np.abs(merged_df[target_column] - (-np.log(10000 * 1e-9))) < threshold\n",
    "            ).astype(int) # Add the dummy variable\n",
    "\n",
    "            ### Note: interaction terms didn't help with the predictions \n",
    "            # interaction_features = [\n",
    "            #     col for col in selected_columns \n",
    "            #     if col in merged_df.columns and col != 'is_imputed'\n",
    "            # ] # Create interaction terms\n",
    "            \n",
    "            # X_interactions = merged_df[interaction_features].multiply(merged_df['is_imputed'], axis=0)\n",
    "            # X_interactions.columns = [f\"{col}_is_imputed\" for col in interaction_features]\n",
    "\n",
    "        #     # Combine features and interaction terms\n",
    "        #     merged_df = pd.concat([merged_df[selected_columns], X_interactions], axis=1)\n",
    "\n",
    "            # Assign the processed DataFrame back to filtered_df for saving\n",
    "            filtered_df = merged_df.drop(columns=[target_column]).set_index('Molecule ChEMBL ID')\n",
    "\n",
    "        # Construct the output file path\n",
    "        file_name = os.path.basename(file_path)  # Extract file name\n",
    "        output_path = os.path.join(output_dir, file_name)\n",
    "        \n",
    "        # Save the filtered DataFrame\n",
    "        filtered_df.to_csv(output_path)\n",
    "        print(f\"Processed file saved to {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
